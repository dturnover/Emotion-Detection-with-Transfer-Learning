{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Upload dataset from Kaggle"
      ],
      "metadata": {
        "id": "K1trW91b_UQP"
      },
      "id": "K1trW91b_UQP"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "id": "r137fkxZ_T_h"
      },
      "id": "r137fkxZ_T_h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For Colab\n",
        "'''''\n",
        "To obtain the Kaggle.json file:\n",
        "\n",
        "1. Go to Kaggle, to your account, Scroll to API section and Click Expire API Token to remove previous tokens\n",
        "\n",
        "2. Click on Create New API Token - It will download kaggle.json file on your machine.\n",
        "\n",
        "3. Go to your Google Colab project file and run the following commands:\n",
        "\n",
        "More info: https://www.kaggle.com/general/74235\n",
        "\n",
        "'''''\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "WDefAjOU_vBs"
      },
      "id": "WDefAjOU_vBs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "RRM-TZcvPYfV"
      },
      "id": "RRM-TZcvPYfV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download ananthu017/emotion-detection-fer\n",
        "# ! kaggle datasets download emotion-detection-fer #For Localhost\n"
      ],
      "metadata": {
        "id": "b8afvt8nYQIx"
      },
      "id": "b8afvt8nYQIx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics torchinfo GPUtil"
      ],
      "metadata": {
        "id": "ociB2gFxBTo-"
      },
      "id": "ociB2gFxBTo-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import random as random\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "import torchmetrics\n",
        "import mlxtend\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import Tuple\n",
        "from typing import Dict\n",
        "from typing import List\n",
        "from timeit import default_timer as timer\n",
        "from matplotlib import patches as mpatches\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import gc\n",
        "\n",
        "from numba import cuda\n",
        "\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from os import listdir                  \n",
        "from os.path import isfile, join"
      ],
      "metadata": {
        "id": "tJxxMmN_BO4j"
      },
      "id": "tJxxMmN_BO4j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "id": "26QjtARzXopt"
      },
      "id": "26QjtARzXopt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec412a2",
      "metadata": {
        "id": "6ec412a2"
      },
      "outputs": [],
      "source": [
        "def extract_data(zipfile_path: Path, destination_path: Path) -> None:\n",
        "    '''Extracts zipfile'''\n",
        "    \n",
        "    if destination_path.is_dir():\n",
        "        print(f\"{destination_path} exists.\")\n",
        "    else:\n",
        "        print(f\"{destination_path} doesn't exist, creating one...\")\n",
        "        destination_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if not os.listdir(destination_path):\n",
        "        with ZipFile(zipfile_path, 'r') as zip:\n",
        "            print(\"Extracting files...\")\n",
        "            zip.extractall(destination_path)\n",
        "\n",
        "            print(\"Extracting finished.\")\n",
        "    else:\n",
        "        print(\"Data already extracted.\")\n",
        "    \n",
        "data_path = Path(\"Kaggle/CV/Emotions_detection\")\n",
        "# zipfile_path = \"/content/emotion-detection-fer.zip\"\n",
        "zipfile_path = \"emotion-detection-fer.zip\" #For Local use\n",
        "\n",
        "images_path = data_path / \"emotions_dataset\"\n",
        "\n",
        "extract_data(zipfile_path, images_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "def print_random_image(images_path: list, seed=None) -> None:\n",
        "    \"\"\"Prints one random photo with details such as class, heigh, width\"\"\"\n",
        "    if seed:\n",
        "        random.seed(seed)\n",
        "\n",
        "    random_image_path = random.choice(images_path)\n",
        "    image_class = random_image_path.parent.stem\n",
        "    image = Image.open(random_image_path)\n",
        "    print(f\"Random image class: {image_class}\")\n",
        "    print(f\"Image height: {image.height}\")\n",
        "    print(f\"Image width: {image.width}\")\n",
        "    plt.imshow(image.convert('P'))\n",
        "\n",
        "    \n",
        "image_path_list = list(images_path.glob(\"*/*/*\"))\n",
        "print_random_image(image_path_list)"
      ],
      "metadata": {
        "id": "0reTEwCDCBoa"
      },
      "id": "0reTEwCDCBoa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing steps: \n",
        "# normalize values\n",
        "# convert to grayscale\n",
        "# create augmented data in an imagedatagenerator\n",
        "# \n",
        "# transfer learning perhaps\n",
        "# CNN with last layer being softmax# preprocessing steps: \n",
        "# normalize values\n",
        "# convert to grayscale\n",
        "# create augmented data in an imagedatagenerator\n",
        "# \n",
        "# transfer learning perhaps\n",
        "# CNN with last layer being softmax"
      ],
      "metadata": {
        "id": "PBG_KEALBx04"
      },
      "id": "PBG_KEALBx04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data"
      ],
      "metadata": {
        "id": "NAKFz2ktRScf"
      },
      "id": "NAKFz2ktRScf"
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose(\n",
        "    [transforms.Resize(size=(224, 224)),\n",
        "     transforms.ToTensor()]\n",
        ")\n",
        "\n",
        "val_transform = transforms.Compose(\n",
        "    [transforms.Resize(size=(224, 224)),\n",
        "     transforms.ToTensor()]\n",
        ")"
      ],
      "metadata": {
        "id": "2xN7-GohRQzq"
      },
      "id": "2xN7-GohRQzq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_transformed_images(images_path: Path,\n",
        "                            transform: transforms,\n",
        "                            n: int=3,\n",
        "                            seed=None) -> None:\n",
        "    \"\"\"Selects random images from a path, transforms them and plots original vs transform\"\"\"\n",
        "    if seed:\n",
        "        random.seed(seed)\n",
        "\n",
        "    if n > 10:\n",
        "        print(\"n shouldn't be higher than 10 due to the size of displayed plot, changing n to 10\")\n",
        "        n = 10\n",
        "\n",
        "    random_image_paths = random.sample(images_path, k=n)\n",
        "\n",
        "    for image_path in random_image_paths:\n",
        "        with Image.open(image_path).convert('RGB') as f:\n",
        "            fig, ax = plt.subplots(nrows=1, ncols=2)\n",
        "            ax[0].imshow(f)\n",
        "            ax[0].set_title(f\"Original\\nsize: {f.size}\")\n",
        "            ax[0].axis(False)\n",
        "\n",
        "            transformed_image = transform(f).permute(1, 2, 0)\n",
        "            ax[1].imshow(transformed_image)\n",
        "            ax[1].set_title(f\"Transformed\\nshape: {transformed_image.shape}\")\n",
        "            ax[1].axis(False)\n",
        "\n",
        "\n",
        "plot_transformed_images(image_path_list, train_transform)"
      ],
      "metadata": {
        "id": "hLbO7QmXShLL"
      },
      "id": "hLbO7QmXShLL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f58a7b1",
      "metadata": {
        "id": "1f58a7b1"
      },
      "outputs": [],
      "source": [
        "train_dir = images_path / \"train\"\n",
        "val_dir = images_path / \"test\"\n",
        "\n",
        "train_data = datasets.ImageFolder(\n",
        "    root=train_dir,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "val_data = datasets.ImageFolder(\n",
        "    root=val_dir,\n",
        "    transform=val_transform\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2a5c52",
      "metadata": {
        "id": "aa2a5c52"
      },
      "source": [
        "### Randomly shuffle the train and test images and simultaneously shuffle the respective labels in unison"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset=val_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "image_batch.shape, label_batch.shape"
      ],
      "metadata": {
        "id": "Rl39hTloUC7p"
      },
      "id": "Rl39hTloUC7p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eec4579",
      "metadata": {
        "id": "2eec4579"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbb0b9ce",
      "metadata": {
        "id": "bbb0b9ce"
      },
      "outputs": [],
      "source": [
        "output_shape = len(train_data.classes)\n",
        "\n",
        "model = torchvision.models.efficientnet_b7().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(\n",
        "    model=model,\n",
        "    input_size=(BATCH_SIZE, 3, 224, 224),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "q8s-fEmUW-DI"
      },
      "id": "q8s-fEmUW-DI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True), \n",
        "    torch.nn.Linear(in_features=2560, \n",
        "                    out_features=output_shape, # same number of output units as our number of classes\n",
        "                    bias=True)).to(device)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iGh20Vo9Wxc5"
      },
      "id": "iGh20Vo9Wxc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de968d85",
      "metadata": {
        "id": "de968d85"
      },
      "outputs": [],
      "source": [
        "def train_step(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    loss_fn: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: torch.device) -> Tuple[float, float]:\n",
        "\n",
        "    model.train()\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_pred = model(X)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        y_pred_class = torch.argmax(y_pred, dim=1)\n",
        "        train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n",
        "\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n",
        "def val_step(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    loss_fn: torch.nn.Module,\n",
        "    device: torch.device) -> Tuple[float, float, torch.Tensor]:\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_acc = 0, 0\n",
        "    y_preds = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            val_pred_logits = model(X)\n",
        "            loss = loss_fn(val_pred_logits, y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            val_pred_labels = torch.argmax(val_pred_logits, dim=1)\n",
        "            val_acc += ((val_pred_labels == y).sum().item() / len(val_pred_labels))\n",
        "            y_preds.append(val_pred_labels.cpu())\n",
        "\n",
        "    val_loss = val_loss / len(dataloader)\n",
        "    val_acc = val_acc / len(dataloader)\n",
        "\n",
        "    y_pred_tensor = torch.cat(y_preds)\n",
        "\n",
        "    return val_loss, val_acc, y_pred_tensor\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: torch.utils.data.DataLoader,\n",
        "    val_dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    loss_fn: torch.nn.Module,\n",
        "    epochs: int,\n",
        "    device: torch.device) -> Tuple[Dict, torch.Tensor]:\n",
        "\n",
        "    results = {\"train_loss\": [],\n",
        "             \"train_acc\": [],\n",
        "             \"val_loss\": [],\n",
        "             \"val_acc\": []}\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(\n",
        "            model=model,\n",
        "            dataloader=train_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        val_loss, val_acc, y_preds = val_step(\n",
        "            model=model,\n",
        "            dataloader=val_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        print(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Train acc: {train_acc:.3f}, Val loss: {val_loss:.3f}, Val acc: {val_acc:.3f}\")\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"val_loss\"].append(val_loss)\n",
        "        results[\"val_acc\"].append(val_acc)\n",
        "\n",
        "    return results, y_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6f89c75",
      "metadata": {
        "id": "e6f89c75"
      },
      "outputs": [],
      "source": [
        "torch.cuda.manual_seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(\n",
        "    params=model.parameters(),\n",
        "    lr=0.01\n",
        ")\n",
        "\n",
        "start_time = timer()\n",
        "\n",
        "model_results, preds = train(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    epochs=EPOCHS,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "end_time = timer()\n",
        "print(f\"Total learning time: {(end_time - start_time):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8733f948",
      "metadata": {
        "id": "8733f948"
      },
      "outputs": [],
      "source": [
        "def plot_curves(results: Dict[str, List[float]]) -> None:\n",
        "    \"\"\"Plots loss and accuracy from a results dictionary.\"\"\"\n",
        "\n",
        "    train_loss = results[\"train_loss\"]\n",
        "    val_loss = results[\"val_loss\"]\n",
        "\n",
        "    train_accuracy = results[\"train_acc\"]\n",
        "    val_accuracy = results[\"val_acc\"]\n",
        "\n",
        "    epochs = range(len(results[\"train_loss\"]))\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_loss, label=\"train_loss\")\n",
        "    plt.plot(epochs, val_loss, label=\"val_loss\")\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracy, label=\"train_accuracy\")\n",
        "    plt.plot(epochs, val_accuracy, label=\"val_accuracy\")\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "plot_curves(model_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9387241e",
      "metadata": {
        "id": "9387241e"
      },
      "outputs": [],
      "source": [
        "def make_predictions(model: torch.nn.Module,\n",
        "                     data: list,\n",
        "                     device: torch.device) -> torch.Tensor:\n",
        "\n",
        "    pred_probs = []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for sample in data:\n",
        "            sample = torch.unsqueeze(sample, dim=0).to(device)\n",
        "            pred_logit = model(sample)\n",
        "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
        "            pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "    return torch.stack(pred_probs)\n",
        "\n",
        "\n",
        "def show_predictions(model: torch.nn.Module,\n",
        "                     device: torch.device,\n",
        "                     val_data: datasets) -> None:\n",
        "\n",
        "    val_samples = []\n",
        "    val_labels = []\n",
        "\n",
        "    for sample, label in random.sample(list(val_data), k=9):\n",
        "        val_samples.append(sample)\n",
        "        val_labels.append(label)\n",
        "\n",
        "    pred_probs = make_predictions(\n",
        "        model=model,\n",
        "        data=val_samples,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    pred_classes = pred_probs.argmax(dim=1)\n",
        "    plt.figure(figsize=(9, 9))\n",
        "    nrows = 3\n",
        "    ncols = 3\n",
        "\n",
        "    for i, sample in enumerate(val_samples):\n",
        "        plt.subplot(nrows, ncols, i+1)\n",
        "        image = sample.squeeze().permute(1, 2, 0)\n",
        "        plt.imshow(image)\n",
        "        pred_label = val_data.classes[pred_classes[i]]\n",
        "        truth_label = val_data.classes[val_labels[i]]\n",
        "        title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "\n",
        "        if pred_label == truth_label:\n",
        "            plt.title(title_text, fontsize=10, c=\"g\")\n",
        "        else:\n",
        "            plt.title(title_text, fontsize=10, c=\"r\")\n",
        "        plt.axis(False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "show_predictions(\n",
        "    model=model,\n",
        "    val_data=val_data,\n",
        "    device=device)"
      ],
      "metadata": {
        "id": "rSBuqJviL1-j"
      },
      "id": "rSBuqJviL1-j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import/install Gradio \n",
        "try:\n",
        "    import gradio as gr\n",
        "except: \n",
        "    !pip -q install gradio\n",
        "    import gradio as gr\n",
        "    \n",
        "print(f\"Gradio version: {gr.__version__}\")"
      ],
      "metadata": {
        "id": "35IS1qnZdmVT"
      },
      "id": "35IS1qnZdmVT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict(inp):\n",
        "  inp = transforms.ToTensor()(inp).unsqueeze(0)\n",
        "  with torch.no_grad():\n",
        "    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n",
        "    confidences = {val_data.classes[i]: float(prediction[i]) for i in range(len(val_data.classes))}    \n",
        "  return confidences"
      ],
      "metadata": {
        "id": "sP_e9lqTmL4C"
      },
      "id": "sP_e9lqTmL4C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\") \n",
        "\n",
        "# Check the device\n",
        "next(iter(model.parameters())).device"
      ],
      "metadata": {
        "id": "CpLb8umcnbdp"
      },
      "id": "CpLb8umcnbdp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_path_list = list(val_dir.glob(\"*/*\"))\n",
        "example_list = [[str(filepath)] for filepath in random.sample(val_path_list, k=3)]\n",
        "example_list"
      ],
      "metadata": {
        "id": "RbmvTPgYrHmK"
      },
      "id": "RbmvTPgYrHmK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Model"
      ],
      "metadata": {
        "id": "yf0vNDPp5rav"
      },
      "id": "yf0vNDPp5rav"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Contains various utility functions for PyTorch model training and saving.\n",
        "\"\"\"\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "    \"\"\"Saves a PyTorch model to a target directory.\n",
        "    Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extension.\n",
        "    Example usage:\n",
        "    save_model(model=model_0,\n",
        "               target_dir=\"models\",\n",
        "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "    \"\"\"\n",
        "    # Create target directory\n",
        "    target_dir_path = Path(target_dir)\n",
        "    target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True)\n",
        "\n",
        "    # Create model save path\n",
        "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "    model_save_path = target_dir_path / model_name\n",
        "\n",
        "    # Save the model state_dict()\n",
        "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "    torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ],
      "metadata": {
        "id": "FdGco0QOKV-F"
      },
      "id": "FdGco0QOKV-F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model=model,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"efficientnet_b0.pth\")"
      ],
      "metadata": {
        "id": "kP_jFwZfJ-0F"
      },
      "id": "kP_jFwZfJ-0F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment Test"
      ],
      "metadata": {
        "id": "PXil8BJgpslT"
      },
      "id": "PXil8BJgpslT"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_effnetb0_model(num_classes:int=7, \n",
        "                          seed:int=42, reid=False):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of classes in the classifier head. \n",
        "            Defaults to 3.\n",
        "        seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): EffNetB0 feature extractor model. \n",
        "        transforms (torchvision.transforms): EffNetB0 image transforms.\n",
        "    \"\"\"\n",
        "    # Create EffNetB0 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b0(weights=weights)\n",
        "\n",
        "    # Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1280, out_features=num_classes),\n",
        "    )\n",
        "    \n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "JmiMe315JxK5"
      },
      "id": "JmiMe315JxK5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms \n",
        "class_names = [\"Happy\", \"Sad\", \"Disgusted\",\"Suprised\",\"Fearful\",\"Angry\",\"Neutral\"]\n",
        "\n",
        "### 2. Model and transforms preparation ###\n",
        "\n",
        "# Create EffNetB2 model\n",
        "effnetb0, effnetb0_transforms = create_effnetb0_model(\n",
        "    num_classes=7, # len(class_names) would also work\n",
        ")\n",
        "\n",
        "# Load saved weights\n",
        "effnetb0.load_state_dict(\n",
        "    torch.load(\n",
        "        f=\"models/efficientnet_b0.pth\",\n",
        "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
        "    )\n",
        ")\n",
        "\n",
        "### 3. Predict function ###\n",
        "\n",
        "# Create predict function\n",
        "\n",
        "def predict(inp):\n",
        "  inp = transforms.ToTensor()(inp).unsqueeze(0)\n",
        "  with torch.no_grad():\n",
        "    prediction = torch.nn.functional.softmax(effnetb0(inp)[0], dim=0)\n",
        "    confidences = {class_names[i]: float(prediction[i]) for i in range(len(class_names))}    \n",
        "  return confidences\n"
      ],
      "metadata": {
        "id": "HW3fOTVlVxOT"
      },
      "id": "HW3fOTVlVxOT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 4. Gradio app ###\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"Emotion Detection App üòÄüòêüò∞üòûü§¢üò≤üò°\"\n",
        "description = \"An EfficientNetB0 computer vision model to classify images of emotions: Happy, Neutral, Sad, fearful, Angry, Suprised, Disgusted.\"\n",
        "article = \"Reference: [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "gr.Interface(fn=predict, \n",
        "             inputs=gr.Image(type=\"pil\"),\n",
        "             outputs=gr.Label(num_top_classes=7),\n",
        "             examples=example_list,\n",
        "                             title=title,\n",
        "                    description=description,\n",
        "                    article=article).launch()\n"
      ],
      "metadata": {
        "id": "U60TEHkIVxKq"
      },
      "id": "U60TEHkIVxKq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploying Model to Production"
      ],
      "metadata": {
        "id": "0j_yrYBBsQae"
      },
      "id": "0j_yrYBBsQae"
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create FoodVision mini demo path\n",
        "foodvision_mini_demo_path = Path(\"demos/emotiondetection_app/\")\n",
        "\n",
        "# Remove files that might already exist there and create new directory\n",
        "if foodvision_mini_demo_path.exists():\n",
        "    shutil.rmtree(foodvision_mini_demo_path)\n",
        "    foodvision_mini_demo_path.mkdir(parents=True, # make the parent folders?\n",
        "                                    exist_ok=True) # create it even if it already exists?\n",
        "else:\n",
        "    # If the file doesn't exist, create it anyway\n",
        "    foodvision_mini_demo_path.mkdir(parents=True, \n",
        "                                    exist_ok=True)\n",
        "    \n",
        "# Check what's in the folder\n",
        "!ls demos/emoitiondetection_app/"
      ],
      "metadata": {
        "id": "RMnr2Euxz6Gk"
      },
      "id": "RMnr2Euxz6Gk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Create an examples directory\n",
        "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\n",
        "foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Collect three random test dataset image paths\n",
        "foodvision_mini_examples = [Path('Kaggle/CV/Emotions_detection/emotions_dataset/test/fearful/im867.png'),\n",
        "                            Path('Kaggle/CV/Emotions_detection/emotions_dataset/test/sad/im90.png'),\n",
        "                            Path('Kaggle/CV/Emotions_detection/emotions_dataset/test/fearful/im175.png')]\n",
        "\n",
        "# 3. Copy the three random images to the examples directory\n",
        "for example in foodvision_mini_examples:\n",
        "    destination = foodvision_mini_examples_path / example.name\n",
        "    print(f\"[INFO] Copying {example} to {destination}\")\n",
        "    shutil.copy2(src=example, dst=destination)"
      ],
      "metadata": {
        "id": "WPiKRfwm9TxB"
      },
      "id": "WPiKRfwm9TxB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get example filepaths in a list of lists\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
        "example_list"
      ],
      "metadata": {
        "id": "XNeq2t7h9mwq"
      },
      "id": "XNeq2t7h9mwq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create a source path for our target model\n",
        "effnetb0_foodvision_mini_model_path = \"models/efficientnetb0.pth\"\n",
        "\n",
        "# Create a destination path for our target model \n",
        "effnetb0_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb0_foodvision_mini_model_path.split(\"/\")[1]\n",
        "\n",
        "# Try to move the file\n",
        "try:\n",
        "    print(f\"[INFO] Attempting to move {effnetb0_foodvision_mini_model_path} to {effnetb0_foodvision_mini_model_destination}\")\n",
        "    \n",
        "    # Move the model\n",
        "    shutil.move(src=effnetb0_foodvision_mini_model_path, \n",
        "                dst=effnetb0_foodvision_mini_model_destination)\n",
        "    \n",
        "    print(f\"[INFO] Model move complete.\")\n",
        "\n",
        "# If the model has already been moved, check if it exists\n",
        "except:\n",
        "    print(f\"[INFO] No model found at {effnetb0_foodvision_mini_model_path}, perhaps its already been moved?\")\n",
        "    print(f\"[INFO] Model exists at {effnetb0_foodvision_mini_model_destination}: {effnetb0_foodvision_mini_model_destination.exists()}\")"
      ],
      "metadata": {
        "id": "LFAXBtyw9yTl"
      },
      "id": "LFAXBtyw9yTl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/emotiondetection_app//model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def create_effnetb0_model(num_classes:int=7, \n",
        "                          seed:int=42):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of classes in the classifier head. \n",
        "            Defaults to 3.\n",
        "        seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): EffNetB0 feature extractor model. \n",
        "        transforms (torchvision.transforms): EffNetB0 image transforms.\n",
        "    \"\"\"\n",
        "    # Create EffNetB0 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b0(weights=weights)\n",
        "\n",
        "    # Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "    \n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "bK9R0eUy93e2"
      },
      "id": "bK9R0eUy93e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/emotiondetection_app/app.py\n",
        "### 1. Imports and class names setup ### \n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Setup class names\n",
        "class_names = [\"Happy\", \"Sad\", \"Disgusted\",\"fearful\",\"Angry\",\"Neutral\"]\n",
        "\n",
        "### 2. Model and transforms preparation ###\n",
        "\n",
        "# Create EffNetB2 model\n",
        "effnetb0, effnetb0_transforms = create_effnetb0_model(\n",
        "    num_classes=7, # len(class_names) would also work\n",
        ")\n",
        "\n",
        "# Load saved weights\n",
        "effnetb0.load_state_dict(\n",
        "    torch.load(\n",
        "        f=\"models/efficientnet_b0.pth\",\n",
        "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
        "    )\n",
        ")\n",
        "\n",
        "### 3. Predict function ###\n",
        "\n",
        "# Create predict function\n",
        "\n",
        "def predict(inp):\n",
        "  inp = transforms.ToTensor()(inp).unsqueeze(0)\n",
        "  with torch.no_grad():\n",
        "    prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n",
        "    confidences = {val_data.classes[i]: float(prediction[i]) for i in range(len(val_data.classes))}    \n",
        "  return confidences\n",
        "\n",
        "\n",
        "### 4. Gradio app ###\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"Emotion Detection App üòÄüòêüò∞üòûü§¢üò≤üò°\"\n",
        "description = \"An EfficientNetB0 computer vision model to classify images of emotions: Happy, Neutral, Sad, fearful, Angry, Suprised, Disgusted.\"\n",
        "article = \"Reference: [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "gr.Interface(fn=predict, \n",
        "             inputs=gr.Image(type=\"pil\"),\n",
        "             outputs=gr.Label(num_top_classes=7),\n",
        "             examples=example_list,\n",
        "                             title=title,\n",
        "                    description=description,\n",
        "                    article=article).launch()"
      ],
      "metadata": {
        "id": "RUlydBaE9-Cl"
      },
      "id": "RUlydBaE9-Cl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/emotiondetection_app/requirements.txt\n",
        "torch==1.12.0\n",
        "torchvision==0.1.9\n",
        "gradio==3.1.4"
      ],
      "metadata": {
        "id": "wc0N8LIo-DBX"
      },
      "id": "wc0N8LIo-DBX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls demos/emotiondetection_app"
      ],
      "metadata": {
        "id": "d_vbQEdr-Ee4"
      },
      "id": "d_vbQEdr-Ee4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change into and then zip the foodvision_mini folder but exclude certain files\n",
        "!cd demos/emotiondetection_app && zip -r ../emotiondetection_app.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
        "\n",
        "# Download the zipped FoodVision Mini app (if running in Google Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(\"demos/emotion_detection.zip\")\n",
        "except:\n",
        "    print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")"
      ],
      "metadata": {
        "id": "nsoYaAvK-IqL"
      },
      "id": "nsoYaAvK-IqL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "* https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer\n",
        "\n",
        "* https://debuggercafe.com/pytorch-pretrained-efficientnet-model-image-classification/\n",
        "\n",
        "* https://www.learnpytorch.io/05_pytorch_going_modular/\n",
        "\n",
        "* https://github.com/mrdbourke/pytorch-deep-learning/blob/main/03_pytorch_computer_vision.ipynb\n",
        "\n",
        "* https://www.kaggle.com/code/saworz/animals-classification-pretrained-vgg16-val94-5\n",
        "\n",
        "* https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b0.html\n",
        "\n",
        "* To run on your local machine/server:: https://research.google.com/colaboratory/local-runtimes.html\n"
      ],
      "metadata": {
        "id": "QZP1-WvjQ9o4"
      },
      "id": "QZP1-WvjQ9o4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}